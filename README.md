# SENTIMENT-ANALYSIS

*COMPANY*: CODTECH IT SOLUTIONS

*NAME*: RUQSAR FIRDOUS

*INTERN ID*: CT04DG2858

*DOMAIN*: DATA ANALYST

*DURATION* : 4 WEEKS

*MENTOR*: NEELA SANTOSH

## ğŸ“Œ **Task 4 â€” Sentiment Analysis of Zomato & Swiggy Play Store Reviews**

For my fourth task, I focused on a very practical and industry-relevant challenge: performing **sentiment analysis** on real user reviews of Indiaâ€™s two leading food delivery apps â€” **Zomato** and **Swiggy**. This task gave me hands-on experience in working with unstructured text data, vectorizing it, training models, evaluating performance, and preparing the results for further visualization and dashboarding.

---

### ğŸ“‚ **Dataset Summary**

The dataset came in the form of **two separate CSV files**:

* `zomato.csv` â€” containing Zomatoâ€™s Play Store reviews.
* `swiggy.csv` â€” containing Swiggyâ€™s Play Store reviews.

Both files had similar structures: `App`, `review_date`, `review_description`, `rating`, `thumbsUpCount`, `developer_response`, `developer_response_date`, and `appVersion`. These columns provide not just the user opinion but also context like when the review was written, whether the developer responded, and which app version the feedback applies to.

---

### ğŸ—‚ï¸ **1ï¸âƒ£ Combining the Data**

First, I loaded both files into **Google Colab** using pandas and combined them into a single DataFrame. Merging them allowed me to handle the entire set of customer feedback together, making it easy to compare Zomato and Swiggy side by side.

---

### ğŸ§¹ **2ï¸âƒ£ Cleaning & Handling Missing Data**

The next step was to inspect the dataset for missing values and duplicates. I found that columns like `developer_response` and `developer_response_date` had a lot of missing entries because not every user receives a reply from the app developers. Instead of dropping these rows or columns â€” which could have removed valuable feedback â€” I filled missing developer responses with **â€˜No Responseâ€™** and missing response dates with **â€˜No Response Dateâ€™**. For the `appVersion` column, I filled missing values with **â€˜Unknown Versionâ€™**.

After this, I dropped any duplicate reviews based on the `review_description` column to ensure data consistency. Finally, I reset the index to keep the DataFrame clean.

---

### ğŸ“ **3ï¸âƒ£ Preprocessing the Text**

Since I was working with user-generated text, cleaning it was essential. I used basic NLP techniques:

* Converted all text to lowercase to maintain consistency.
* Removed punctuation, special characters, and numbers.
* Removed common stopwords to focus on the important words.
* Tokenized the text and joined it back for vectorization.

This cleaned version of the reviews was stored in a new column `cleaned_review`.

---

### ğŸ·ï¸ **4ï¸âƒ£ Creating Sentiment Labels**

To perform supervised learning, I converted the numerical star ratings into sentiment categories:

* Ratings **4â€“5** â†’ **Positive**
* Rating **3** â†’ **Neutral**
* Ratings **1â€“2** â†’ **Negative**

This `sentiment` column became my target variable for training the model.

---

### ğŸ”¢ **5ï¸âƒ£ Vectorizing the Text**

I transformed the cleaned text into numerical features using **TF-IDF Vectorization**. I limited the features to the top 5000 words to balance accuracy and resource usage, which is crucial when working in Colab with limited RAM.

---

### âœ‚ï¸ **6ï¸âƒ£ Splitting the Data**

I split the data into training and test sets, ensuring the model would be evaluated on unseen data. This gives a realistic measure of how well the model would generalize to new reviews.

---

### ğŸ¤– **7ï¸âƒ£ Training Multiple Models**

I experimented with three models:

* **Logistic Regression**: Simple yet very effective, giving me the highest accuracy of \~89%.
* **Random Forest**: Took longer and gave \~83% accuracy.
* **XGBoost**: Gave \~85% accuracy, but needed more runtime.

After testing, I selected **Logistic Regression** as my final model because of its balance between speed, accuracy, and ease of deployment.

---

### âœ… **8ï¸âƒ£ Making Predictions**

Using the trained Logistic Regression model, I made sentiment predictions for the test data. Since the predicted labels needed to match the original format, I inverse-transformed the encoded predictions back to the text labels: **Positive**, **Neutral**, and **Negative**.

---

### ğŸ’¾ **9ï¸âƒ£ Exporting the Results**

Finally, I carefully merged the predictions back into my original DataFrame by matching only the test indices. This ensured that my final dataset shows both the true and predicted sentiment for test rows without overwriting the original data. I exported this as `final_reviews_with_sentiment.csv` and saved the model accuracy in a separate `.txt` file for reference.

---

### ğŸ¯ **Outcome & Next Steps**

With this pipeline, I successfully built a practical **NLP classification workflow** â€” from data cleaning and vectorization to training, evaluation, and exporting results. The next logical step is to use this enriched dataset to create an **interactive sentiment dashboard** in Power BI or Tableau, where we can visually analyze user moods, detect trends by app version, track thumbs-up counts, and see how developer responses impact sentiment.

---

### ğŸ’¡ **Key Learnings**

This task strengthened my confidence in:

* Handling messy, real-world text data.
* Applying NLP preprocessing techniques.
* Training and evaluating multiple models.
* Understanding the trade-offs between model complexity, runtime, and accuracy.
* Preparing data for further business intelligence use cases.

---

By doing this, I not only improved my practical data science workflow but also laid the foundation for a clear, actionable **customer sentiment analysis** â€” which is invaluable for companies like Swiggy or Zomato to understand customer pain points, respond better, and improve their services.

---
